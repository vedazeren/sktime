

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>sktime.benchmarking.evaluation &mdash; sktime 0.4.0 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/fields.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../_static/sktime-favicon.ico"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home" alt="Documentation Home"> sktime
          

          
          </a>

          
            
            
              <div class="version">
                0.4.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../how_to_get_started.html">How to get started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_reference.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../how_to_contribute.html">How to contribute</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about.html">About us</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributors.html">Contributors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../code_of_conduct.html">Code of Conduct</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">sktime</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>sktime.benchmarking.evaluation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="module-sktime.benchmarking.evaluation">
<span id="sktime-benchmarking-evaluation"></span><h1>sktime.benchmarking.evaluation<a class="headerlink" href="#module-sktime.benchmarking.evaluation" title="Permalink to this headline">¶</a></h1>
<dl class="py class">
<dt id="sktime.benchmarking.evaluation.Evaluator">
<em class="property">class </em><code class="sig-prename descclassname">sktime.benchmarking.evaluation.</code><code class="sig-name descname">Evaluator</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">results</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/sktime/benchmarking/evaluation.html#Evaluator"><span class="viewcode-link">[source]</span></a><a class="reference external" href="https://github.com/alan-turing-institute/sktime/blob/v0.4.0/sktime/benchmarking/evaluation.py#L18-L624"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sktime.benchmarking.evaluation.Evaluator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.8)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Analyze results of machine learning experiments.</p>
<dl class="py method">
<dt id="sktime.benchmarking.evaluation.Evaluator.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">metric</span></em>, <em class="sig-param"><span class="n">train_or_test</span><span class="o">=</span><span class="default_value">'test'</span></em>, <em class="sig-param"><span class="n">cv_fold</span><span class="o">=</span><span class="default_value">'all'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/sktime/benchmarking/evaluation.html#Evaluator.evaluate"><span class="viewcode-link">[source]</span></a><a class="reference external" href="https://github.com/alan-turing-institute/sktime/blob/v0.4.0/sktime/benchmarking/evaluation.py#L58-L126"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sktime.benchmarking.evaluation.Evaluator.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the average prediction error per estimator as well as the
prediction error achieved by each
estimator on individual datasets.</p>
</dd></dl>

<dl class="py method">
<dt id="sktime.benchmarking.evaluation.Evaluator.friedman_test">
<code class="sig-name descname">friedman_test</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">metric_name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/sktime/benchmarking/evaluation.html#Evaluator.friedman_test"><span class="viewcode-link">[source]</span></a><a class="reference external" href="https://github.com/alan-turing-institute/sktime/blob/v0.4.0/sktime/benchmarking/evaluation.py#L362-L385"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sktime.benchmarking.evaluation.Evaluator.friedman_test" title="Permalink to this definition">¶</a></dt>
<dd><p>The Friedman test is a non-parametric statistical test used to
detect differences
in treatments across multiple test attempts. The procedure involves
ranking each row (or block) together,
then considering the values of ranks by columns.
Implementation used:
<a class="reference external" href="https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.friedmanchisquare.html">scipy.stats</a>.</p>
</dd></dl>

<dl class="py method">
<dt id="sktime.benchmarking.evaluation.Evaluator.metric_names">
<em class="property">property </em><code class="sig-name descname">metric_names</code><a class="reference external" href="https://github.com/alan-turing-institute/sktime/blob/v0.4.0/sktime/benchmarking/evaluation.py"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sktime.benchmarking.evaluation.Evaluator.metric_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sktime.benchmarking.evaluation.Evaluator.metrics">
<em class="property">property </em><code class="sig-name descname">metrics</code><a class="reference external" href="https://github.com/alan-turing-institute/sktime/blob/v0.4.0/sktime/benchmarking/evaluation.py"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sktime.benchmarking.evaluation.Evaluator.metrics" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sktime.benchmarking.evaluation.Evaluator.metrics_by_strategy">
<em class="property">property </em><code class="sig-name descname">metrics_by_strategy</code><a class="reference external" href="https://github.com/alan-turing-institute/sktime/blob/v0.4.0/sktime/benchmarking/evaluation.py"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sktime.benchmarking.evaluation.Evaluator.metrics_by_strategy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sktime.benchmarking.evaluation.Evaluator.metrics_by_strategy_dataset">
<em class="property">property </em><code class="sig-name descname">metrics_by_strategy_dataset</code><a class="reference external" href="https://github.com/alan-turing-institute/sktime/blob/v0.4.0/sktime/benchmarking/evaluation.py"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sktime.benchmarking.evaluation.Evaluator.metrics_by_strategy_dataset" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sktime.benchmarking.evaluation.Evaluator.nemenyi">
<code class="sig-name descname">nemenyi</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">metric_name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/sktime/benchmarking/evaluation.html#Evaluator.nemenyi"><span class="viewcode-link">[source]</span></a><a class="reference external" href="https://github.com/alan-turing-institute/sktime/blob/v0.4.0/sktime/benchmarking/evaluation.py#L387-L410"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sktime.benchmarking.evaluation.Evaluator.nemenyi" title="Permalink to this definition">¶</a></dt>
<dd><p>Post-hoc test run if the <cite>friedman_test</cite> reveals statistical
significance.
For more information see <a class="reference external" href="https://en.wikipedia.org/wiki/Nemenyi_test">Nemenyi test</a>.
Implementation used <a class="reference external" href="https://github.com/maximtrp/scikit-posthocs">scikit-posthocs</a>.</p>
</dd></dl>

<dl class="py method">
<dt id="sktime.benchmarking.evaluation.Evaluator.plot_boxplots">
<code class="sig-name descname">plot_boxplots</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">metric_name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/sktime/benchmarking/evaluation.html#Evaluator.plot_boxplots"><span class="viewcode-link">[source]</span></a><a class="reference external" href="https://github.com/alan-turing-institute/sktime/blob/v0.4.0/sktime/benchmarking/evaluation.py#L128-L141"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sktime.benchmarking.evaluation.Evaluator.plot_boxplots" title="Permalink to this definition">¶</a></dt>
<dd><p>Box plot of metric</p>
</dd></dl>

<dl class="py method">
<dt id="sktime.benchmarking.evaluation.Evaluator.plot_critical_difference_diagram">
<code class="sig-name descname">plot_critical_difference_diagram</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">metric_name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">0.1</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/sktime/benchmarking/evaluation.html#Evaluator.plot_critical_difference_diagram"><span class="viewcode-link">[source]</span></a><a class="reference external" href="https://github.com/alan-turing-institute/sktime/blob/v0.4.0/sktime/benchmarking/evaluation.py#L412-L565"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sktime.benchmarking.evaluation.Evaluator.plot_critical_difference_diagram" title="Permalink to this definition">¶</a></dt>
<dd><p>Plot critical difference diagrams</p>
<p>original implementation by Aaron Bostrom, modified by Markus Löning</p>
</dd></dl>

<dl class="py method">
<dt id="sktime.benchmarking.evaluation.Evaluator.rank">
<code class="sig-name descname">rank</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">metric_name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">ascending</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/sktime/benchmarking/evaluation.html#Evaluator.rank"><span class="viewcode-link">[source]</span></a><a class="reference external" href="https://github.com/alan-turing-institute/sktime/blob/v0.4.0/sktime/benchmarking/evaluation.py#L143-L166"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sktime.benchmarking.evaluation.Evaluator.rank" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the average ranks based on the performance of each
estimator on each dataset</p>
</dd></dl>

<dl class="py method">
<dt id="sktime.benchmarking.evaluation.Evaluator.ranksum_test">
<code class="sig-name descname">ranksum_test</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">metric_name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/sktime/benchmarking/evaluation.html#Evaluator.ranksum_test"><span class="viewcode-link">[source]</span></a><a class="reference external" href="https://github.com/alan-turing-institute/sktime/blob/v0.4.0/sktime/benchmarking/evaluation.py#L248-L289"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sktime.benchmarking.evaluation.Evaluator.ranksum_test" title="Permalink to this definition">¶</a></dt>
<dd><p>Non-parametric test for testing consistent differences between pairs
of obeservations.
The test counts the number of observations that are greater, smaller
and equal to the mean
<a class="reference external" href="http://en.wikipedia.org/wiki/Wilcoxon_rank-sum_test">http://en.wikipedia.org/wiki/Wilcoxon_rank-sum_test</a>.</p>
</dd></dl>

<dl class="py method">
<dt id="sktime.benchmarking.evaluation.Evaluator.sign_test">
<code class="sig-name descname">sign_test</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">metric_name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/sktime/benchmarking/evaluation.html#Evaluator.sign_test"><span class="viewcode-link">[source]</span></a><a class="reference external" href="https://github.com/alan-turing-institute/sktime/blob/v0.4.0/sktime/benchmarking/evaluation.py#L209-L246"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sktime.benchmarking.evaluation.Evaluator.sign_test" title="Permalink to this definition">¶</a></dt>
<dd><p>Non-parametric test for test for consistent differences between
pairs of observations.
See <a class="reference external" href="https://en.wikipedia.org/wiki/Sign_test">https://en.wikipedia.org/wiki/Sign_test</a> for details about
the test and
<a class="reference external" href="https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.binom_test.html">https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.binom_test.html</a>
for details about the scipy implementation.</p>
</dd></dl>

<dl class="py method">
<dt id="sktime.benchmarking.evaluation.Evaluator.t_test">
<code class="sig-name descname">t_test</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">metric_name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/sktime/benchmarking/evaluation.html#Evaluator.t_test"><span class="viewcode-link">[source]</span></a><a class="reference external" href="https://github.com/alan-turing-institute/sktime/blob/v0.4.0/sktime/benchmarking/evaluation.py#L168-L207"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sktime.benchmarking.evaluation.Evaluator.t_test" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs t-test on all possible combinations between the estimators.</p>
</dd></dl>

<dl class="py method">
<dt id="sktime.benchmarking.evaluation.Evaluator.t_test_with_bonferroni_correction">
<code class="sig-name descname">t_test_with_bonferroni_correction</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">metric_name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">0.05</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/sktime/benchmarking/evaluation.html#Evaluator.t_test_with_bonferroni_correction"><span class="viewcode-link">[source]</span></a><a class="reference external" href="https://github.com/alan-turing-institute/sktime/blob/v0.4.0/sktime/benchmarking/evaluation.py#L291-L313"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sktime.benchmarking.evaluation.Evaluator.t_test_with_bonferroni_correction" title="Permalink to this definition">¶</a></dt>
<dd><p>correction used to counteract multiple comparissons
<a class="reference external" href="https://en.wikipedia.org/wiki/Bonferroni_correction">https://en.wikipedia.org/wiki/Bonferroni_correction</a></p>
</dd></dl>

<dl class="py method">
<dt id="sktime.benchmarking.evaluation.Evaluator.wilcoxon_test">
<code class="sig-name descname">wilcoxon_test</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">metric_name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/sktime/benchmarking/evaluation.html#Evaluator.wilcoxon_test"><span class="viewcode-link">[source]</span></a><a class="reference external" href="https://github.com/alan-turing-institute/sktime/blob/v0.4.0/sktime/benchmarking/evaluation.py#L315-L360"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sktime.benchmarking.evaluation.Evaluator.wilcoxon_test" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test">http://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test</a>
<a class="reference external" href="https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test">Wilcoxon signed-rank test</a>.
Tests whether two  related paired samples come from the same
distribution.
In particular, it tests whether the distribution of the differences
x-y is symmetric about zero</p>
</dd></dl>

</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2019 - 2020, sktime developers (BSD-3-Clause License)

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>